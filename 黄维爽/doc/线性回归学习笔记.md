# 线性回归

## 回归的概念：

监督学习分为**回归**和**分类**：

- **回归**  (Regression、Prediction）     *如何预测上海浦东的房价? 标签连续未来的股票市场走向?* 
- **分类**  (Classification)——— 标签离散              *身高1.85m，体重100kg的男人穿什么尺码的T恤？ 根据肿瘤的体积、患者的年龄来判断良性或恶性?*			

## 线性回归的概念

- **线性回归**（Linear Regression） 是一种通过属性的线性组合来进行预测的**线性模型**，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化



## 线性模型

#### 一、 线性模型

- 给定由n个属性描述的列向量$f(\mathbf{x})={(x^{(1)};x^{(2)};...;x^{(n)})}$，其中 $x^{(j)}$是$\textbf{x}$在第 j 个属性的取值。线性模型即为通过对属性进行线性组合的函数，即$f(\mathbf{x})=w_0+w_1x^{(1)}+...+w_nx^{(n)}$写成向量形式如下：$f(\textbf{x})=\mathbf{w}^\mathrm{T}\mathbf{x}$其中列向量$\mathbf{w}=(w_0;w_1;...;w_n)$，列向量$\mathbf{x}=(1;x^{(1)};...;x^{(n)})$。列向量$\mathbf{w}$确定后，模型随之确定。

#### 二 、线性模型求解

- 对于给定的数据集$\mathbf{D}=\left \{ (\mathbf{x_1},y_1),(\mathbf{x_2},y_2),...,(\mathbf{x_m},y_m)\right \}$,其中$\mathbf{x_i}=(x_i^{(1)};...;x_i^{(n)})$，$y_i$为第i个实例的实际值。“线性回归”试图学得一个线性模型以尽可能准确地预测实例的输出值，使之接近实际值。

- 关键问题是如何衡量两者之间的误差。这里采用均方误差作为性能度量，即利用最小二乘法来进行参数估计。
  $$
  \underset{\mathbf{w}}{argmin}\mathbf{\mathit{J}}(\mathbf{w})=\frac{1}{2m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)^2
  $$
  实际上在高斯分布假设前提下，用极大似然函数来进行参数估计，可以得出上述目标,推导过程如下。

  根据中心极限定理，认为误差项 $\mathbf\xi $服从均值为零的高斯分布
  $$
  P(\xi_i)=\frac{1}{\sqrt{2\pi }\sigma }exp(-\frac{\xi_i^2}{2\sigma ^2})
  $$
  
  $$
  P(y_i|\mathbf{x_i};\mathbf{w})=\frac{1}{\sqrt{2\pi }\sigma }exp(-\frac{(y_i-\mathbf{x}_i)^2}{2\sigma ^2})​
  $$
  

  由上可得似然函数，
  $$
  \mathbf{L}(\mathbf{w})=\prod_{i=1}^{m}P(y_i|\mathbf{x_i};\mathbf{w})
  $$
  

  取对数，得对数似然函数
  $$
  \mathbf{L}(\mathbf{w})=mlog\frac{1}{\sqrt{2\pi }\sigma }-\frac{1}{2\sigma ^2}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)^2
  $$
  

  对上式取极大值等价于下式取极小值
  $$
  \mathbf{\mathit{J}}(\mathbf{w})=\frac{1}{2m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)^2
  $$
  

  推导完毕。

- 模型求解方法：矩阵直接求解和梯度下降法。

  1 .**正规方程法**:

  对于数据集D中的每个实例组成一个矩阵，矩阵形式如下：
  $$
  \mathbf{X}=\begin{pmatrix} 1 & x_1^{(1)} & ... & x_1^{(n)}\\ 1& x_2^{(1)} & ... & x_2^{(n)}\\ .& . & ... & .\\ 1& x_m^{(1)} & . & x_m^{(n)} \end{pmatrix}=\begin{pmatrix} 1 &\mathbf{x}_1^T \\ 1 &\mathbf{x}_2^T\\ .& .\\ 1& \mathbf{x}_m^T \end{pmatrix}
  $$
  

  对应的实际值写成列向量形式$\mathbf{y}=(y_1;y_2;...;y_m)$,则有 $    \mathbf{\hat{w}}^*=\underset{\hat{\mathbf{w}}}{argmin}(\mathbf{y}-\mathbf{X\hat{w}})^T(\mathbf{y}-\mathbf{X\hat{w}})$

  上式argmin后面部分对$\hat{\mathbf{w}}$求导，令之等于零，得到$\mathbf{\hat{w}}^*=(\mathbf{X^TX})^{-1}\mathbf{X^T}\mathbf{y}$,  令$\mathbf{\hat{x}}_i=(1;\mathbf{x}_i)$

  从而得到线性模型$f(\mathbf{\hat{x}}_i)=\mathbf{\hat{x}}_i^T\mathbf{\hat{w}}^*$,或者$f(\mathbf{\hat{x}}_i)=\mathbf{\hat{w}}^{*T}\mathbf{\hat{x}}_i$。

  但是，现实情况中$\mathbf{X^TX}$往往**不可逆**(数学现象)，也就是实际上的**过拟合**(由于学习能力过于强大，把训练样本自身的一些特点当作了所以潜在样本都会具有的一般性质，导致泛化性能，也就是举一反三的能力下降)，通常原因有两种，一是高度共线性；二是数据特征过多而训练数据较少，此时可以通过**正则化**来解决。

  [^1]: 会再写一篇专门介绍这些

  2 .梯度下降法

  梯度下降是一种常用的一阶优化方法，是求解无约束优化问题的经典方法之一。对于连续可微函数上某一点，有各个方向导数，沿梯度方向的方向导数达到最大值，也就是说,梯度的方向是函数在这点增长最快的方向。

  因此，我们可以得到如下结论：函数在某点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。

  所以我们可以沿反梯度方向不断一步一步迭代，得到局部极小点。当目标函数为凸函数时，局部极小点就是全局最小点，此时梯度下降法可确保收敛到全局最优解。

  将损失函数对列向量$\mathbf{w}$求导，得到$w_j$的偏导：
  $$
  \frac{\partial \mathbf{J(w)}}{\partial w_j}=\frac{\partial }{\partial w_j}\frac{1}{2m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)^2=\frac{1}{m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)\mathbf{x}_i^{(j)},j=0,1,2,...,n
  $$
  

  然后对各个分量都以下面形式更新$w_j$：
  $$
  w_j=w_j-\alpha\frac{1}{m} \sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)\mathbf{x}_i^{(j)}
  $$
  

  从公式可以看出对于每一个分量进行一次迭代时计算了所有训练样本数据，这种称为**批量梯度下降**。因此在数据量很大的时候，每次迭代都要遍历训练集一遍，开销会很大。

  为改善上述情况，可以在每次迭代仅选择一个训练样本去计算代价函数的梯度，然后更新参数。即使是大规模数据集，随机梯度下降法也会很快收敛。这种方法称为**随机梯度下降**。

  此时有，$w_j=w_j-\alpha (f(\mathbf{x}_i)-y_i)\mathbf{x}_i^{(j)}$

  关于随机梯度下降、批量梯度下降与小批量随机梯度下降的特征不再赘述。

  #### 三 运算向量化表示。

  - 梯度求解过程中用到了求和，代码实现时用循坏太过繁琐，可借助矩阵运算，简洁迅速。
    $$
    \mathbf{x}_i=\begin{bmatrix} 1\\ x_i^{(1)}\\ \vdots \\ x_i^{(n)} \end{bmatrix},y=\begin{bmatrix} y_1\\ y_2\\ \vdots \\ y_m \end{bmatrix},\mathbf{w}=\begin{bmatrix} w_0\\ w_1\\ \vdots \\ w_n \end{bmatrix},X=\begin{bmatrix} 1 & x_1^{(1)} & x_1^{(2)} & \cdots & x_1^{(n)}\\ 1 
    & x_2^{(1)} & x_2^{(1)} & \cdots & x_2^{(1)}\\ \vdots &\vdots &\vdots &\cdots &\vdots \\ 1 & x_m^{(1)} &x_m^{(1)} & \cdots & x_m^{(1)} \end{bmatrix}
    $$

    $$
    \begin{bmatrix} f(x_1)-y_1\\ f(x_2)-y_2\\ \vdots \\ f(x_m)-y_m \end{bmatrix}=\begin{bmatrix} \mathbf{w^Tx}_1-y_1\\ \mathbf{w^Tx}_2-y_2\\ \vdots \\ \mathbf{w^Tx}_m-y_m \end{bmatrix}=X\mathbf{w}-y
    $$

    

    损失函数为:    
    $$
    J(\mathbf{w})=\frac{1}{2m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)^2
    $$
    

    对$w_j$求梯度:
    $$
    \frac{\partial \mathbf{J(w)}}{\partial w_j}=\frac{\partial }{\partial w_j}\frac{1}{2m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)^2=\frac{1}{m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)\mathbf{x}_i^{(j)} , j=0,1,2,...,n
    $$
    
    $$
    \frac{\partial J}{\partial \mathbf{w}}=\begin{bmatrix} \frac{\partial J}{\partial w^{(0)}}\\ \frac{\partial J}{\partial w^{(1)}}\\ \vdots \\ \frac{\partial J}{\partial w^{(n)}} \end{bmatrix}=\frac{1}{m}\mathbf{X^T}(\mathbf{Xw}-y)
    $$

  

  #### 补充：

  - L1正则化下的损失函数

  $$
  \mathbf{\mathit{J}}(\mathbf{w})=\frac{1}{2m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)^2+\lambda \left \| \mathbf{w} \right \|_{1}
  $$

  

  ​       其中$\left \| \mathbf{w} \right \|_{1}=\sum w_j$

  

  - L2正则化下的损失函数

  $$
  \mathbf{\mathit{J}}(\mathbf{w})=\frac{1}{2m}\sum_{i=1}^{m}(f(\mathbf{x}_i)-y_i)^2+\lambda/2 \left \| \mathbf{w} \right \|_{2}^2
  $$

  

  ​       其中$\left \| W \right \|_{2}=\sqrt{\sum w_j^2}$

  

  加入L1正则化后称为**Lasso回归**，加入L2正则化称为**Ridge回归**，其中 $\lambda$为模型的超参数。

  L1 regularizer ： 它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。

  L2 regularizer ：使得模型的解偏向于 norm 较小的 W，通过限制 W 的 norm 的大小实现了对模型空间的限制，从而在一定程度上避免了过拟合（overfitting ）。不过 ridge regression 并不具有产生稀疏解的能力，得到的系数 仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。




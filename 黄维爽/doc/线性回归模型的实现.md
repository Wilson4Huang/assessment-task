# 线性回归模型的实现

## 1.项目实现过程

### 前期：

- 基础知识的学习（当然，我有很多的知识是项目实现过程中学到的）：

​	        · 强化数学基础（高数，线代，概率论），懂得线性回归数学底层逻辑（这个十分重要）

​	        · 学习编程语言基础，把思路用编程语言传递给计算机，让计算机实现“学习”

-  明确机器学习工作流程：

​            · 获取数据（现有）

​            · 数据预处理、特征工程

​            · 算法代码实现、训练模型与优化

​            · 模型检验（可视化与评估）

### 中期：

-  数据预处理：处理缺失值、处理异常值

-  代码的编写：根据学习过的线性回归内容（代价函数，梯度）用python实现，注意代码的命名规则与语法（让代码具有可读性与易懂性）
-  查找资料，咨询师兄，增加知识来源渠道（当我没有思路的时候，这些是好方法）

### 后期：

- 模型的优化与调参
- 优化数据预处理的代码
- 交流与学习
- 总结出现的问题

## 2.学到的知识

-   一句业界广泛流传的术语：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已 —— 这句话使我醍醐灌顶：原来数据处理如此的重要！！！而且在以后可以调库的情况下，模型和算法占机器学习过程的十分之一，大部分时间都是用在数据预处理和特征工程上的
-  要明确线性回归模型实现的框架，知道要做什么，不要做什么，思路很重要，有了思路就可以比较快地写出代码、实现模型（不然就会像无头苍蝇乱撞，会走很多弯路，惨痛的教训）
- 一些代码编写的技巧：比如说不同的代码都可以实现一个删除缺失值的功能，还有删除某一列的操作，还有某种功能的实现比较繁琐，会有很多代码与方法可以简化操作，比如说均值填充缺失值不用一列一列操作，python有内置函数可以直接填充全部列
- 数学知识：求偏导，对于复杂的多元函数求偏导，矩阵的乘法，一些矩阵运算的规则（线代的知识就当是复习了），一些线性回归知识，如正规方程法和梯度下降法的底层逻辑与直观理解（最小二乘法的矩阵形式，矩阵求偏导，迭代，类比梯度下降为下山，学习率为布长，梯度为方向导数最大值即下山方向，找到全局最低点意味着cost函数收敛）

## 3.过程中遇到的问题与解决方法

- 刚开始我太着急，急于得出数据取验证，导致数据处理不充分，只做了缺失值的处理，跳过了很多必要的处理，如：处理异常值等等。导致最后的结果强差人意，模型检验的效果很差
- 忘记特征工程的重要操作：归一化。导致梯度下降得到的权重溢出（第一座大山），后面经过查找资料与请教得到了解答，然后得到了结果去检验时效果仍然不好（得分为0.3）
- 这时我就没啥思路了，代码检查过，没问题，数据处理，没问题，到底是哪里出了问题呢？这个疑惑整整困扰了我几天，没有进展以至于我中途去做了一点点的爬虫
- 首先我想到这个有可能是过拟合，去学习了如何处理过拟合的方法，将梯度加入正则项之后，效果反减不增（我已经逐渐崩溃）
- 重新整列思路，把过程在草稿纸演练一遍，终于发现问题所在：我训练集做了归一化，得到的权重是由归一化的测试集和y值产生的，但是我的测试集没有归一化（第二座大山），这样的测试集与权重相乘的到预测值y_pred肯定与y值大相庭径。把测试集归一化之后，果然分数提高了（0.7）
- 还有没有方法可以使结果更好一些呢？于是我针对数据处理和模型优化下手了，测试了多种组合（处理异常值与归一化，不处理异常值与归一化，处理异常值与不归一化，不处理异常值与不归一化），发现每次的分数都不一样，其中处理异常值与不归一化的效果最好（得分0.8），因此我得到一个结论，并不是模型都有一套模板，不同的数据会有不一样的方法，因此实践出真知，我的结果也是在无数次检验中得出来的（这就是探索精神，虽然很繁琐，但是结果是好的！）

